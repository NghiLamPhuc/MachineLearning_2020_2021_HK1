{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        4521 non-null   int64 \n",
      " 1   job        4521 non-null   object\n",
      " 2   marital    4521 non-null   object\n",
      " 3   education  4521 non-null   object\n",
      " 4   default    4521 non-null   object\n",
      " 5   balance    4521 non-null   int64 \n",
      " 6   housing    4521 non-null   object\n",
      " 7   loan       4521 non-null   object\n",
      " 8   contact    4521 non-null   object\n",
      " 9   day        4521 non-null   int64 \n",
      " 10  month      4521 non-null   object\n",
      " 11  duration   4521 non-null   int64 \n",
      " 12  campaign   4521 non-null   int64 \n",
      " 13  pdays      4521 non-null   int64 \n",
      " 14  previous   4521 non-null   int64 \n",
      " 15  poutcome   4521 non-null   object\n",
      " 16  y          4521 non-null   object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 600.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         job  marital  education default  balance housing loan  \\\n",
       "0   30  unemployed  married    primary      no     1787      no   no   \n",
       "1   33    services  married  secondary      no     4789     yes  yes   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  cellular   19   oct        79         1     -1         0  unknown  no  \n",
       "1  cellular   11   may       220         1    339         4  failure  no  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_data = pd.read_csv(\"bank.csv\", sep=';')\n",
    "raw_data.info()\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         job  marital  education default  balance housing loan  \\\n",
       "0   30  unemployed  married    primary      no     1787      no   no   \n",
       "1   33    services  married  secondary      no     4789     yes  yes   \n",
       "\n",
       "    contact  day month  campaign  pdays  previous poutcome   y  \n",
       "0  cellular   19   oct         1     -1         0  unknown  no  \n",
       "1  cellular   11   may         1    339         4  failure  no  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = raw_data.drop(columns='duration')\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder # Chuyển thuộc tính dạng category thành số.\n",
    "def categoryToInt(data: pd.DataFrame):\n",
    "    listAttrCate = ['job', 'marital', 'education', 'default', 'housing', 'month', 'loan', 'contact', 'poutcome', 'y']\n",
    "    new_data = data.copy()\n",
    "    LE = LabelEncoder()\n",
    "    for attr in listAttrCate:\n",
    "        new_data[attr] = LE.fit_transform(new_data[attr])\n",
    "    return new_data\n",
    "\n",
    "new_data = categoryToInt(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3986, 16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z = np.abs(stats.zscore(new_data))\n",
    "revmove_outlier_data = new_data[(z < 3).all(axis=1)]\n",
    "revmove_outlier_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]:  30 10 1 0 0 1787 0 0 0 19 10 1 -1 0 3\n",
      "len X[0]:  15\n",
      "Class 0:  3551\n",
      "Class 1:  435\n"
     ]
    }
   ],
   "source": [
    "X = revmove_outlier_data.values[:,:-1]\n",
    "y = revmove_outlier_data.values[:,-1:].flatten()# flatten copy còn ravel không copy\n",
    "listAttrName = revmove_outlier_data.columns.tolist() # lấy danh sách tên thuộc tính (vẽ trong decision tree)\n",
    "print('X[0]: ', *X[0])\n",
    "print('len X[0]: ', len(X[0]))\n",
    "print('Class 0: ', len(y[y==0]))\n",
    "print('Class 1: ', len(y[y==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3587, 15)\n",
      "(399, 15)\n",
      "(3587,)\n",
      "(399,)\n",
      "X_train[0] :  31 4 1 2 0 1170 0 0 0 20 1 4 -1 0 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "# lấy 10% là test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print('X_train[0] : ', *X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/51237635/difference-between-standard-scaler-and-minmaxscaler#:~:text=StandardScaler%20removes%20the%20mean%20and%20scales%20the%20data%20to%20unit%20variance.&text=MinMaxScaler%20rescales%20the%20data%20set,the%20transformed%20number%20of%20households.\n",
    "## https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n",
    "# Scale sau bước train_test_split để không rò rỉ dữ liệu?\n",
    "\n",
    "#Scale MinMax\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n",
    "scaler_m = MinMaxScaler()\n",
    "# scaler_s = StandardScaler()\n",
    "# scaler_r = RobustScaler()\n",
    "\n",
    "scaler_m.fit(X_train)\n",
    "X_train_m = scaler_m.transform(X_train)\n",
    "\n",
    "# scaler_s.fit(X_train)\n",
    "# X_train_s = scaler_s.transform(X_train)\n",
    "\n",
    "# scaler_r.fit(X_train)\n",
    "# X_train_r = scaler_r.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  31    4    1    2    0 1170    0    0    0   20    1    4   -1    0\n",
      "     3]\n",
      " [  58    1    0    0    0 8218    1    0    0   31    5   10   -1    0\n",
      "     3]]\n",
      "[[0.22641509 0.36363636 0.5        0.66666667 0.         0.24051468\n",
      "  0.         0.         0.         0.63333333 0.09090909 0.27272727\n",
      "  0.         0.         1.        ]\n",
      " [0.73584906 0.09090909 0.         0.         0.         0.82184098\n",
      "  1.         0.         0.         1.         0.45454545 0.81818182\n",
      "  0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:2])\n",
    "print(X_train_m[:2])\n",
    "# print(X_train_s[:2])\n",
    "# print(X_train_r[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35394533\n",
      "Iteration 2, loss = 0.33278773\n",
      "Iteration 3, loss = 0.32902829\n",
      "Iteration 4, loss = 0.32625070\n",
      "Iteration 5, loss = 0.32618354\n",
      "Iteration 6, loss = 0.32520926\n",
      "Iteration 7, loss = 0.32290967\n",
      "Iteration 8, loss = 0.32280295\n",
      "Iteration 9, loss = 0.32183683\n",
      "Iteration 10, loss = 0.32285378\n",
      "Iteration 11, loss = 0.32209219\n",
      "Iteration 12, loss = 0.32103220\n",
      "Iteration 13, loss = 0.31906042\n",
      "Iteration 14, loss = 0.32162778\n",
      "Iteration 15, loss = 0.31952555\n",
      "Iteration 16, loss = 0.32163356\n",
      "Iteration 17, loss = 0.31817926\n",
      "Iteration 18, loss = 0.31869994\n",
      "Iteration 19, loss = 0.31790462\n",
      "Iteration 20, loss = 0.31878025\n",
      "Iteration 21, loss = 0.31827018\n",
      "Iteration 22, loss = 0.31806472\n",
      "Iteration 23, loss = 0.31835469\n",
      "Iteration 24, loss = 0.32003793\n",
      "Iteration 25, loss = 0.31845686\n",
      "Iteration 26, loss = 0.31746849\n",
      "Iteration 27, loss = 0.31827349\n",
      "Iteration 28, loss = 0.31736313\n",
      "Iteration 29, loss = 0.31695642\n",
      "Iteration 30, loss = 0.31840686\n",
      "Iteration 31, loss = 0.31767516\n",
      "Iteration 32, loss = 0.31899555\n",
      "Iteration 33, loss = 0.31757673\n",
      "Iteration 34, loss = 0.31679145\n",
      "Iteration 35, loss = 0.31787208\n",
      "Iteration 36, loss = 0.31832081\n",
      "Iteration 37, loss = 0.31656300\n",
      "Iteration 38, loss = 0.31715733\n",
      "Iteration 39, loss = 0.31762449\n",
      "Iteration 40, loss = 0.31754497\n",
      "Iteration 41, loss = 0.31732906\n",
      "Iteration 42, loss = 0.31860300\n",
      "Iteration 43, loss = 0.31811310\n",
      "Iteration 44, loss = 0.31678406\n",
      "Iteration 45, loss = 0.31696009\n",
      "Iteration 46, loss = 0.31792449\n",
      "Iteration 47, loss = 0.31697884\n",
      "Iteration 48, loss = 0.31765662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "#identity scale minmax\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_identity = MLPClassifier(max_iter=500, batch_size=200, verbose=True, learning_rate_init=0.01, activation='identity', hidden_layer_sizes=(10, 10), random_state=42).fit(X_train_m, y_train)\n",
    "# model_identity = MLPClassifier(verbose=True, learning_rate='adaptive', activation='identity', hidden_layer_sizes=(10, 10), max_iter=500, random_state=42).fit(X_train_m, y_train)\n",
    "y_pred_iden_m = model_identity.predict(X_test)\n",
    "# 3587/200=17.935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04831270\n",
      "Iteration 2, loss = 0.59432271\n",
      "Iteration 3, loss = 0.43096289\n",
      "Iteration 4, loss = 0.36826812\n",
      "Iteration 5, loss = 0.35108962\n",
      "Iteration 6, loss = 0.34795907\n",
      "Iteration 7, loss = 0.34764583\n",
      "Iteration 8, loss = 0.34761918\n",
      "Iteration 9, loss = 0.34745277\n",
      "Iteration 10, loss = 0.34729384\n",
      "Iteration 11, loss = 0.34706019\n",
      "Iteration 12, loss = 0.34666461\n",
      "Iteration 13, loss = 0.34612350\n",
      "Iteration 14, loss = 0.34541003\n",
      "Iteration 15, loss = 0.34443062\n",
      "Iteration 16, loss = 0.34329711\n",
      "Iteration 17, loss = 0.34153379\n",
      "Iteration 18, loss = 0.33965440\n",
      "Iteration 19, loss = 0.33750807\n",
      "Iteration 20, loss = 0.33543267\n",
      "Iteration 21, loss = 0.33289501\n",
      "Iteration 22, loss = 0.33069653\n",
      "Iteration 23, loss = 0.32870475\n",
      "Iteration 24, loss = 0.32695377\n",
      "Iteration 25, loss = 0.32542615\n",
      "Iteration 26, loss = 0.32433086\n",
      "Iteration 27, loss = 0.32364145\n",
      "Iteration 28, loss = 0.32240715\n",
      "Iteration 29, loss = 0.32229621\n",
      "Iteration 30, loss = 0.32159034\n",
      "Iteration 31, loss = 0.32057395\n",
      "Iteration 32, loss = 0.32016090\n",
      "Iteration 33, loss = 0.31999213\n",
      "Iteration 34, loss = 0.31921365\n",
      "Iteration 35, loss = 0.31937457\n",
      "Iteration 36, loss = 0.31913129\n",
      "Iteration 37, loss = 0.31918573\n",
      "Iteration 38, loss = 0.31872418\n",
      "Iteration 39, loss = 0.31828876\n",
      "Iteration 40, loss = 0.31822318\n",
      "Iteration 41, loss = 0.31836132\n",
      "Iteration 42, loss = 0.31811290\n",
      "Iteration 43, loss = 0.31878886\n",
      "Iteration 44, loss = 0.31772722\n",
      "Iteration 45, loss = 0.31677049\n",
      "Iteration 46, loss = 0.31714029\n",
      "Iteration 47, loss = 0.31646886\n",
      "Iteration 48, loss = 0.31646318\n",
      "Iteration 49, loss = 0.31605276\n",
      "Iteration 50, loss = 0.31583739\n",
      "Iteration 51, loss = 0.31565835\n",
      "Iteration 52, loss = 0.31597674\n",
      "Iteration 53, loss = 0.31512780\n",
      "Iteration 54, loss = 0.31556288\n",
      "Iteration 55, loss = 0.31463347\n",
      "Iteration 56, loss = 0.31415186\n",
      "Iteration 57, loss = 0.31391294\n",
      "Iteration 58, loss = 0.31351934\n",
      "Iteration 59, loss = 0.31328512\n",
      "Iteration 60, loss = 0.31301008\n",
      "Iteration 61, loss = 0.31434665\n",
      "Iteration 62, loss = 0.31338874\n",
      "Iteration 63, loss = 0.31346759\n",
      "Iteration 64, loss = 0.31353170\n",
      "Iteration 65, loss = 0.31237227\n",
      "Iteration 66, loss = 0.31217856\n",
      "Iteration 67, loss = 0.31102866\n",
      "Iteration 68, loss = 0.31245468\n",
      "Iteration 69, loss = 0.31209232\n",
      "Iteration 70, loss = 0.31251266\n",
      "Iteration 71, loss = 0.31227833\n",
      "Iteration 72, loss = 0.31217749\n",
      "Iteration 73, loss = 0.31046169\n",
      "Iteration 74, loss = 0.30998979\n",
      "Iteration 75, loss = 0.31033541\n",
      "Iteration 76, loss = 0.30952154\n",
      "Iteration 77, loss = 0.30938810\n",
      "Iteration 78, loss = 0.31281663\n",
      "Iteration 79, loss = 0.30951539\n",
      "Iteration 80, loss = 0.30986640\n",
      "Iteration 81, loss = 0.30892080\n",
      "Iteration 82, loss = 0.30884896\n",
      "Iteration 83, loss = 0.30871952\n",
      "Iteration 84, loss = 0.30988253\n",
      "Iteration 85, loss = 0.30907762\n",
      "Iteration 86, loss = 0.30845985\n",
      "Iteration 87, loss = 0.30803061\n",
      "Iteration 88, loss = 0.31085365\n",
      "Iteration 89, loss = 0.30829307\n",
      "Iteration 90, loss = 0.30793810\n",
      "Iteration 91, loss = 0.30862534\n",
      "Iteration 92, loss = 0.30768597\n",
      "Iteration 93, loss = 0.30829601\n",
      "Iteration 94, loss = 0.30772859\n",
      "Iteration 95, loss = 0.30740256\n",
      "Iteration 96, loss = 0.30665411\n",
      "Iteration 97, loss = 0.30645187\n",
      "Iteration 98, loss = 0.30807271\n",
      "Iteration 99, loss = 0.30669768\n",
      "Iteration 100, loss = 0.30753153\n",
      "Iteration 101, loss = 0.30640951\n",
      "Iteration 102, loss = 0.30658918\n",
      "Iteration 103, loss = 0.30720826\n",
      "Iteration 104, loss = 0.30695416\n",
      "Iteration 105, loss = 0.30654659\n",
      "Iteration 106, loss = 0.30657383\n",
      "Iteration 107, loss = 0.30613009\n",
      "Iteration 108, loss = 0.30639858\n",
      "Iteration 109, loss = 0.30580965\n",
      "Iteration 110, loss = 0.30556899\n",
      "Iteration 111, loss = 0.30616265\n",
      "Iteration 112, loss = 0.30593968\n",
      "Iteration 113, loss = 0.30545991\n",
      "Iteration 114, loss = 0.30528133\n",
      "Iteration 115, loss = 0.30530181\n",
      "Iteration 116, loss = 0.30623061\n",
      "Iteration 117, loss = 0.30856094\n",
      "Iteration 118, loss = 0.30641844\n",
      "Iteration 119, loss = 0.30580610\n",
      "Iteration 120, loss = 0.30744390\n",
      "Iteration 121, loss = 0.30524829\n",
      "Iteration 122, loss = 0.30575123\n",
      "Iteration 123, loss = 0.30621248\n",
      "Iteration 124, loss = 0.30569876\n",
      "Iteration 125, loss = 0.30552229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "#sigmoid scale minmax\n",
    "model_logistic = MLPClassifier(verbose=True, learning_rate_init=0.01,activation='logistic', hidden_layer_sizes=(10, 10), max_iter=500, random_state=42).fit(X_train_m, y_train)\n",
    "y_pred_logistic_m = model_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38769062\n",
      "Iteration 2, loss = 0.34838279\n",
      "Iteration 3, loss = 0.34096257\n",
      "Iteration 4, loss = 0.33173115\n",
      "Iteration 5, loss = 0.32784379\n",
      "Iteration 6, loss = 0.32359972\n",
      "Iteration 7, loss = 0.32236160\n",
      "Iteration 8, loss = 0.31987817\n",
      "Iteration 9, loss = 0.31972531\n",
      "Iteration 10, loss = 0.31834698\n",
      "Iteration 11, loss = 0.31926045\n",
      "Iteration 12, loss = 0.31856963\n",
      "Iteration 13, loss = 0.31744847\n",
      "Iteration 14, loss = 0.31889181\n",
      "Iteration 15, loss = 0.31700080\n",
      "Iteration 16, loss = 0.31746218\n",
      "Iteration 17, loss = 0.31536842\n",
      "Iteration 18, loss = 0.31538398\n",
      "Iteration 19, loss = 0.31502485\n",
      "Iteration 20, loss = 0.31490878\n",
      "Iteration 21, loss = 0.31461313\n",
      "Iteration 22, loss = 0.31379618\n",
      "Iteration 23, loss = 0.31620685\n",
      "Iteration 24, loss = 0.31523391\n",
      "Iteration 25, loss = 0.31192168\n",
      "Iteration 26, loss = 0.31210845\n",
      "Iteration 27, loss = 0.31247740\n",
      "Iteration 28, loss = 0.31127182\n",
      "Iteration 29, loss = 0.31021466\n",
      "Iteration 30, loss = 0.31111616\n",
      "Iteration 31, loss = 0.30955996\n",
      "Iteration 32, loss = 0.30917674\n",
      "Iteration 33, loss = 0.30744234\n",
      "Iteration 34, loss = 0.30701280\n",
      "Iteration 35, loss = 0.30674085\n",
      "Iteration 36, loss = 0.30809070\n",
      "Iteration 37, loss = 0.30696995\n",
      "Iteration 38, loss = 0.30780329\n",
      "Iteration 39, loss = 0.30714854\n",
      "Iteration 40, loss = 0.30573854\n",
      "Iteration 41, loss = 0.30360427\n",
      "Iteration 42, loss = 0.30496768\n",
      "Iteration 43, loss = 0.30406871\n",
      "Iteration 44, loss = 0.30393163\n",
      "Iteration 45, loss = 0.30287093\n",
      "Iteration 46, loss = 0.30316144\n",
      "Iteration 47, loss = 0.30156752\n",
      "Iteration 48, loss = 0.30250614\n",
      "Iteration 49, loss = 0.30261983\n",
      "Iteration 50, loss = 0.30153253\n",
      "Iteration 51, loss = 0.30098135\n",
      "Iteration 52, loss = 0.29957179\n",
      "Iteration 53, loss = 0.29991804\n",
      "Iteration 54, loss = 0.29868353\n",
      "Iteration 55, loss = 0.30087681\n",
      "Iteration 56, loss = 0.29999764\n",
      "Iteration 57, loss = 0.29921899\n",
      "Iteration 58, loss = 0.29866968\n",
      "Iteration 59, loss = 0.29666543\n",
      "Iteration 60, loss = 0.29677228\n",
      "Iteration 61, loss = 0.29825464\n",
      "Iteration 62, loss = 0.29798180\n",
      "Iteration 63, loss = 0.29645293\n",
      "Iteration 64, loss = 0.29636533\n",
      "Iteration 65, loss = 0.29610597\n",
      "Iteration 66, loss = 0.29799650\n",
      "Iteration 67, loss = 0.29465167\n",
      "Iteration 68, loss = 0.29531179\n",
      "Iteration 69, loss = 0.29559260\n",
      "Iteration 70, loss = 0.29363936\n",
      "Iteration 71, loss = 0.29496361\n",
      "Iteration 72, loss = 0.29465283\n",
      "Iteration 73, loss = 0.29504010\n",
      "Iteration 74, loss = 0.29305882\n",
      "Iteration 75, loss = 0.29319862\n",
      "Iteration 76, loss = 0.29266943\n",
      "Iteration 77, loss = 0.29217680\n",
      "Iteration 78, loss = 0.29388980\n",
      "Iteration 79, loss = 0.29401917\n",
      "Iteration 80, loss = 0.29416857\n",
      "Iteration 81, loss = 0.29249003\n",
      "Iteration 82, loss = 0.29072001\n",
      "Iteration 83, loss = 0.29045399\n",
      "Iteration 84, loss = 0.29234748\n",
      "Iteration 85, loss = 0.29331384\n",
      "Iteration 86, loss = 0.29005665\n",
      "Iteration 87, loss = 0.29025245\n",
      "Iteration 88, loss = 0.29450627\n",
      "Iteration 89, loss = 0.29044928\n",
      "Iteration 90, loss = 0.28950639\n",
      "Iteration 91, loss = 0.28987544\n",
      "Iteration 92, loss = 0.28809048\n",
      "Iteration 93, loss = 0.28920460\n",
      "Iteration 94, loss = 0.29021074\n",
      "Iteration 95, loss = 0.28980846\n",
      "Iteration 96, loss = 0.28745273\n",
      "Iteration 97, loss = 0.28756978\n",
      "Iteration 98, loss = 0.28728998\n",
      "Iteration 99, loss = 0.28647185\n",
      "Iteration 100, loss = 0.28634435\n",
      "Iteration 101, loss = 0.28591679\n",
      "Iteration 102, loss = 0.28560709\n",
      "Iteration 103, loss = 0.28665606\n",
      "Iteration 104, loss = 0.28624016\n",
      "Iteration 105, loss = 0.28499626\n",
      "Iteration 106, loss = 0.28562166\n",
      "Iteration 107, loss = 0.28457513\n",
      "Iteration 108, loss = 0.28417312\n",
      "Iteration 109, loss = 0.28329474\n",
      "Iteration 110, loss = 0.28300352\n",
      "Iteration 111, loss = 0.28353783\n",
      "Iteration 112, loss = 0.28411254\n",
      "Iteration 113, loss = 0.28373783\n",
      "Iteration 114, loss = 0.28284744\n",
      "Iteration 115, loss = 0.28174045\n",
      "Iteration 116, loss = 0.28164297\n",
      "Iteration 117, loss = 0.28693255\n",
      "Iteration 118, loss = 0.28424381\n",
      "Iteration 119, loss = 0.28014760\n",
      "Iteration 120, loss = 0.28501777\n",
      "Iteration 121, loss = 0.28182055\n",
      "Iteration 122, loss = 0.28044864\n",
      "Iteration 123, loss = 0.27971332\n",
      "Iteration 124, loss = 0.27980469\n",
      "Iteration 125, loss = 0.28079781\n",
      "Iteration 126, loss = 0.27877015\n",
      "Iteration 127, loss = 0.27788109\n",
      "Iteration 128, loss = 0.27743544\n",
      "Iteration 129, loss = 0.27744876\n",
      "Iteration 130, loss = 0.27850023\n",
      "Iteration 131, loss = 0.28011885\n",
      "Iteration 132, loss = 0.27799805\n",
      "Iteration 133, loss = 0.27833251\n",
      "Iteration 134, loss = 0.27826302\n",
      "Iteration 135, loss = 0.27658934\n",
      "Iteration 136, loss = 0.27610401\n",
      "Iteration 137, loss = 0.27576738\n",
      "Iteration 138, loss = 0.27537670\n",
      "Iteration 139, loss = 0.27572879\n",
      "Iteration 140, loss = 0.27507468\n",
      "Iteration 141, loss = 0.27501866\n",
      "Iteration 142, loss = 0.27498014\n",
      "Iteration 143, loss = 0.27421655\n",
      "Iteration 144, loss = 0.27397048\n",
      "Iteration 145, loss = 0.27455001\n",
      "Iteration 146, loss = 0.27483595\n",
      "Iteration 147, loss = 0.27280547\n",
      "Iteration 148, loss = 0.27234658\n",
      "Iteration 149, loss = 0.27339909\n",
      "Iteration 150, loss = 0.27520493\n",
      "Iteration 151, loss = 0.27145981\n",
      "Iteration 152, loss = 0.27044180\n",
      "Iteration 153, loss = 0.27182977\n",
      "Iteration 154, loss = 0.27165841\n",
      "Iteration 155, loss = 0.27342810\n",
      "Iteration 156, loss = 0.27122238\n",
      "Iteration 157, loss = 0.27009957\n",
      "Iteration 158, loss = 0.26831800\n",
      "Iteration 159, loss = 0.26996357\n",
      "Iteration 160, loss = 0.27182308\n",
      "Iteration 161, loss = 0.27093516\n",
      "Iteration 162, loss = 0.27095032\n",
      "Iteration 163, loss = 0.26763431\n",
      "Iteration 164, loss = 0.26944375\n",
      "Iteration 165, loss = 0.26888324\n",
      "Iteration 166, loss = 0.26745133\n",
      "Iteration 167, loss = 0.26540456\n",
      "Iteration 168, loss = 0.26671941\n",
      "Iteration 169, loss = 0.26608302\n",
      "Iteration 170, loss = 0.26601227\n",
      "Iteration 171, loss = 0.26596509\n",
      "Iteration 172, loss = 0.26658700\n",
      "Iteration 173, loss = 0.26712719\n",
      "Iteration 174, loss = 0.26444905\n",
      "Iteration 175, loss = 0.26525382\n",
      "Iteration 176, loss = 0.26420436\n",
      "Iteration 177, loss = 0.26446202\n",
      "Iteration 178, loss = 0.26442475\n",
      "Iteration 179, loss = 0.26423187\n",
      "Iteration 180, loss = 0.26563475\n",
      "Iteration 181, loss = 0.26462829\n",
      "Iteration 182, loss = 0.26206397\n",
      "Iteration 183, loss = 0.26570635\n",
      "Iteration 184, loss = 0.26494046\n",
      "Iteration 185, loss = 0.26425503\n",
      "Iteration 186, loss = 0.26130902\n",
      "Iteration 187, loss = 0.26142919\n",
      "Iteration 188, loss = 0.26070068\n",
      "Iteration 189, loss = 0.26093517\n",
      "Iteration 190, loss = 0.26088025\n",
      "Iteration 191, loss = 0.26113177\n",
      "Iteration 192, loss = 0.25924420\n",
      "Iteration 193, loss = 0.26004657\n",
      "Iteration 194, loss = 0.25911083\n",
      "Iteration 195, loss = 0.25804150\n",
      "Iteration 196, loss = 0.25714985\n",
      "Iteration 197, loss = 0.26266036\n",
      "Iteration 198, loss = 0.25797576\n",
      "Iteration 199, loss = 0.25827754\n",
      "Iteration 200, loss = 0.25778278\n",
      "Iteration 201, loss = 0.26044304\n",
      "Iteration 202, loss = 0.25839945\n",
      "Iteration 203, loss = 0.25794723\n",
      "Iteration 204, loss = 0.25842210\n",
      "Iteration 205, loss = 0.25527144\n",
      "Iteration 206, loss = 0.25520889\n",
      "Iteration 207, loss = 0.25588438\n",
      "Iteration 208, loss = 0.25461075\n",
      "Iteration 209, loss = 0.25630018\n",
      "Iteration 210, loss = 0.25421564\n",
      "Iteration 211, loss = 0.25487555\n",
      "Iteration 212, loss = 0.25280879\n",
      "Iteration 213, loss = 0.25182057\n",
      "Iteration 214, loss = 0.25452401\n",
      "Iteration 215, loss = 0.25190270\n",
      "Iteration 216, loss = 0.25292239\n",
      "Iteration 217, loss = 0.25344158\n",
      "Iteration 218, loss = 0.25682558\n",
      "Iteration 219, loss = 0.25131636\n",
      "Iteration 220, loss = 0.25156673\n",
      "Iteration 221, loss = 0.25109922\n",
      "Iteration 222, loss = 0.25226133\n",
      "Iteration 223, loss = 0.25155001\n",
      "Iteration 224, loss = 0.25240544\n",
      "Iteration 225, loss = 0.24875872\n",
      "Iteration 226, loss = 0.24843501\n",
      "Iteration 227, loss = 0.24798867\n",
      "Iteration 228, loss = 0.24839062\n",
      "Iteration 229, loss = 0.24910677\n",
      "Iteration 230, loss = 0.24899292\n",
      "Iteration 231, loss = 0.24857238\n",
      "Iteration 232, loss = 0.24806061\n",
      "Iteration 233, loss = 0.24657145\n",
      "Iteration 234, loss = 0.24672449\n",
      "Iteration 235, loss = 0.24648942\n",
      "Iteration 236, loss = 0.24688627\n",
      "Iteration 237, loss = 0.24812762\n",
      "Iteration 238, loss = 0.24823478\n",
      "Iteration 239, loss = 0.24633602\n",
      "Iteration 240, loss = 0.24673369\n",
      "Iteration 241, loss = 0.24526659\n",
      "Iteration 242, loss = 0.24583419\n",
      "Iteration 243, loss = 0.24581923\n",
      "Iteration 244, loss = 0.24386293\n",
      "Iteration 245, loss = 0.24462525\n",
      "Iteration 246, loss = 0.24487462\n",
      "Iteration 247, loss = 0.24738471\n",
      "Iteration 248, loss = 0.24613159\n",
      "Iteration 249, loss = 0.24474581\n",
      "Iteration 250, loss = 0.24530107\n",
      "Iteration 251, loss = 0.24494391\n",
      "Iteration 252, loss = 0.24483753\n",
      "Iteration 253, loss = 0.24609280\n",
      "Iteration 254, loss = 0.24289121\n",
      "Iteration 255, loss = 0.24338234\n",
      "Iteration 256, loss = 0.24530865\n",
      "Iteration 257, loss = 0.24526732\n",
      "Iteration 258, loss = 0.24255320\n",
      "Iteration 259, loss = 0.24317796\n",
      "Iteration 260, loss = 0.24594019\n",
      "Iteration 261, loss = 0.24242182\n",
      "Iteration 262, loss = 0.24269315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 263, loss = 0.24227675\n",
      "Iteration 264, loss = 0.24129690\n",
      "Iteration 265, loss = 0.24162651\n",
      "Iteration 266, loss = 0.24298128\n",
      "Iteration 267, loss = 0.24283237\n",
      "Iteration 268, loss = 0.24296018\n",
      "Iteration 269, loss = 0.24016672\n",
      "Iteration 270, loss = 0.24032761\n",
      "Iteration 271, loss = 0.24147797\n",
      "Iteration 272, loss = 0.23957247\n",
      "Iteration 273, loss = 0.24192000\n",
      "Iteration 274, loss = 0.24016473\n",
      "Iteration 275, loss = 0.23915023\n",
      "Iteration 276, loss = 0.23806343\n",
      "Iteration 277, loss = 0.23956304\n",
      "Iteration 278, loss = 0.23830041\n",
      "Iteration 279, loss = 0.23848085\n",
      "Iteration 280, loss = 0.24049485\n",
      "Iteration 281, loss = 0.24105643\n",
      "Iteration 282, loss = 0.23970492\n",
      "Iteration 283, loss = 0.23798975\n",
      "Iteration 284, loss = 0.23811714\n",
      "Iteration 285, loss = 0.23795014\n",
      "Iteration 286, loss = 0.23769384\n",
      "Iteration 287, loss = 0.23892448\n",
      "Iteration 288, loss = 0.23989386\n",
      "Iteration 289, loss = 0.24044600\n",
      "Iteration 290, loss = 0.23648855\n",
      "Iteration 291, loss = 0.23633555\n",
      "Iteration 292, loss = 0.23779956\n",
      "Iteration 293, loss = 0.23881378\n",
      "Iteration 294, loss = 0.23710066\n",
      "Iteration 295, loss = 0.23849990\n",
      "Iteration 296, loss = 0.23497883\n",
      "Iteration 297, loss = 0.23513501\n",
      "Iteration 298, loss = 0.23571485\n",
      "Iteration 299, loss = 0.23681471\n",
      "Iteration 300, loss = 0.23546808\n",
      "Iteration 301, loss = 0.23515001\n",
      "Iteration 302, loss = 0.23682970\n",
      "Iteration 303, loss = 0.23523443\n",
      "Iteration 304, loss = 0.23447479\n",
      "Iteration 305, loss = 0.23310245\n",
      "Iteration 306, loss = 0.23454947\n",
      "Iteration 307, loss = 0.23487311\n",
      "Iteration 308, loss = 0.23576718\n",
      "Iteration 309, loss = 0.23601494\n",
      "Iteration 310, loss = 0.23399666\n",
      "Iteration 311, loss = 0.23364899\n",
      "Iteration 312, loss = 0.23518126\n",
      "Iteration 313, loss = 0.23517884\n",
      "Iteration 314, loss = 0.23409219\n",
      "Iteration 315, loss = 0.23303039\n",
      "Iteration 316, loss = 0.23355359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "#tanh scale minmax\n",
    "model_tanh = MLPClassifier(verbose=True, learning_rate_init=0.01, activation='tanh', hidden_layer_sizes=(10, 10), max_iter=500, random_state=42).fit(X_train_m, y_train)\n",
    "y_pred_tanh_m = model_tanh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.96050828\n",
      "Iteration 2, loss = 0.55443566\n",
      "Iteration 3, loss = 0.35176082\n",
      "Iteration 4, loss = 0.33480668\n",
      "Iteration 5, loss = 0.33223207\n",
      "Iteration 6, loss = 0.32894130\n",
      "Iteration 7, loss = 0.32703443\n",
      "Iteration 8, loss = 0.32586133\n",
      "Iteration 9, loss = 0.32491734\n",
      "Iteration 10, loss = 0.32440516\n",
      "Iteration 11, loss = 0.32357996\n",
      "Iteration 12, loss = 0.32330142\n",
      "Iteration 13, loss = 0.32280701\n",
      "Iteration 14, loss = 0.32252181\n",
      "Iteration 15, loss = 0.32211081\n",
      "Iteration 16, loss = 0.32263518\n",
      "Iteration 17, loss = 0.32176169\n",
      "Iteration 18, loss = 0.32115234\n",
      "Iteration 19, loss = 0.32091102\n",
      "Iteration 20, loss = 0.32043199\n",
      "Iteration 21, loss = 0.31937108\n",
      "Iteration 22, loss = 0.31764808\n",
      "Iteration 23, loss = 0.31737160\n",
      "Iteration 24, loss = 0.31490543\n",
      "Iteration 25, loss = 0.31330811\n",
      "Iteration 26, loss = 0.31215796\n",
      "Iteration 27, loss = 0.31195849\n",
      "Iteration 28, loss = 0.31105212\n",
      "Iteration 29, loss = 0.30954111\n",
      "Iteration 30, loss = 0.31054025\n",
      "Iteration 31, loss = 0.30998964\n",
      "Iteration 32, loss = 0.30892590\n",
      "Iteration 33, loss = 0.30793945\n",
      "Iteration 34, loss = 0.30787960\n",
      "Iteration 35, loss = 0.30782202\n",
      "Iteration 36, loss = 0.30776773\n",
      "Iteration 37, loss = 0.30796208\n",
      "Iteration 38, loss = 0.30820793\n",
      "Iteration 39, loss = 0.30688826\n",
      "Iteration 40, loss = 0.30636993\n",
      "Iteration 41, loss = 0.30587477\n",
      "Iteration 42, loss = 0.30641838\n",
      "Iteration 43, loss = 0.30624499\n",
      "Iteration 44, loss = 0.30547159\n",
      "Iteration 45, loss = 0.30482114\n",
      "Iteration 46, loss = 0.30471342\n",
      "Iteration 47, loss = 0.30460914\n",
      "Iteration 48, loss = 0.30512894\n",
      "Iteration 49, loss = 0.30475036\n",
      "Iteration 50, loss = 0.30421888\n",
      "Iteration 51, loss = 0.30350491\n",
      "Iteration 52, loss = 0.30254590\n",
      "Iteration 53, loss = 0.30185686\n",
      "Iteration 54, loss = 0.30239003\n",
      "Iteration 55, loss = 0.30268579\n",
      "Iteration 56, loss = 0.30117995\n",
      "Iteration 57, loss = 0.30115470\n",
      "Iteration 58, loss = 0.30070543\n",
      "Iteration 59, loss = 0.30014047\n",
      "Iteration 60, loss = 0.29930354\n",
      "Iteration 61, loss = 0.30017774\n",
      "Iteration 62, loss = 0.30093387\n",
      "Iteration 63, loss = 0.30006259\n",
      "Iteration 64, loss = 0.29998209\n",
      "Iteration 65, loss = 0.29983022\n",
      "Iteration 66, loss = 0.29970577\n",
      "Iteration 67, loss = 0.29879876\n",
      "Iteration 68, loss = 0.29933275\n",
      "Iteration 69, loss = 0.29909613\n",
      "Iteration 70, loss = 0.29998487\n",
      "Iteration 71, loss = 0.29902409\n",
      "Iteration 72, loss = 0.29836063\n",
      "Iteration 73, loss = 0.29860762\n",
      "Iteration 74, loss = 0.29669380\n",
      "Iteration 75, loss = 0.29663033\n",
      "Iteration 76, loss = 0.29732661\n",
      "Iteration 77, loss = 0.29675990\n",
      "Iteration 78, loss = 0.29694293\n",
      "Iteration 79, loss = 0.29801710\n",
      "Iteration 80, loss = 0.29651634\n",
      "Iteration 81, loss = 0.29654177\n",
      "Iteration 82, loss = 0.29565133\n",
      "Iteration 83, loss = 0.29536950\n",
      "Iteration 84, loss = 0.29644910\n",
      "Iteration 85, loss = 0.29629231\n",
      "Iteration 86, loss = 0.29500908\n",
      "Iteration 87, loss = 0.29572544\n",
      "Iteration 88, loss = 0.30013423\n",
      "Iteration 89, loss = 0.29689633\n",
      "Iteration 90, loss = 0.29618946\n",
      "Iteration 91, loss = 0.29569059\n",
      "Iteration 92, loss = 0.29451245\n",
      "Iteration 93, loss = 0.29504275\n",
      "Iteration 94, loss = 0.29413431\n",
      "Iteration 95, loss = 0.29529535\n",
      "Iteration 96, loss = 0.29411811\n",
      "Iteration 97, loss = 0.29403019\n",
      "Iteration 98, loss = 0.29446632\n",
      "Iteration 99, loss = 0.29365904\n",
      "Iteration 100, loss = 0.29352324\n",
      "Iteration 101, loss = 0.29447519\n",
      "Iteration 102, loss = 0.29301936\n",
      "Iteration 103, loss = 0.29313492\n",
      "Iteration 104, loss = 0.29332323\n",
      "Iteration 105, loss = 0.29331289\n",
      "Iteration 106, loss = 0.29427446\n",
      "Iteration 107, loss = 0.29417084\n",
      "Iteration 108, loss = 0.29307576\n",
      "Iteration 109, loss = 0.29179357\n",
      "Iteration 110, loss = 0.29137357\n",
      "Iteration 111, loss = 0.29155570\n",
      "Iteration 112, loss = 0.29202409\n",
      "Iteration 113, loss = 0.29188088\n",
      "Iteration 114, loss = 0.29172662\n",
      "Iteration 115, loss = 0.29045984\n",
      "Iteration 116, loss = 0.29215080\n",
      "Iteration 117, loss = 0.29654360\n",
      "Iteration 118, loss = 0.29349789\n",
      "Iteration 119, loss = 0.29157667\n",
      "Iteration 120, loss = 0.29541270\n",
      "Iteration 121, loss = 0.29462592\n",
      "Iteration 122, loss = 0.29038323\n",
      "Iteration 123, loss = 0.29202475\n",
      "Iteration 124, loss = 0.28979734\n",
      "Iteration 125, loss = 0.28949094\n",
      "Iteration 126, loss = 0.28929502\n",
      "Iteration 127, loss = 0.28977425\n",
      "Iteration 128, loss = 0.29032213\n",
      "Iteration 129, loss = 0.28973040\n",
      "Iteration 130, loss = 0.29045748\n",
      "Iteration 131, loss = 0.29069109\n",
      "Iteration 132, loss = 0.28871089\n",
      "Iteration 133, loss = 0.28925915\n",
      "Iteration 134, loss = 0.28914581\n",
      "Iteration 135, loss = 0.28786538\n",
      "Iteration 136, loss = 0.28669575\n",
      "Iteration 137, loss = 0.28708690\n",
      "Iteration 138, loss = 0.28627371\n",
      "Iteration 139, loss = 0.28877326\n",
      "Iteration 140, loss = 0.28826698\n",
      "Iteration 141, loss = 0.28744982\n",
      "Iteration 142, loss = 0.28735465\n",
      "Iteration 143, loss = 0.28685324\n",
      "Iteration 144, loss = 0.28670479\n",
      "Iteration 145, loss = 0.28937175\n",
      "Iteration 146, loss = 0.28706194\n",
      "Iteration 147, loss = 0.28711274\n",
      "Iteration 148, loss = 0.28910701\n",
      "Iteration 149, loss = 0.28753927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "#relu scale minmax\n",
    "model_relu = MLPClassifier(verbose=True, learning_rate_init=0.01, activation='relu', hidden_layer_sizes=(10, 10), max_iter=500, random_state=42).fit(X_train_m, y_train)\n",
    "y_pred_relu_m = model_relu.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------identity\n",
      "accuracy:\n",
      " 25.81453634085213\n",
      "[[ 33 291]\n",
      " [  5  70]]\n",
      "[[ 70   5]\n",
      " [291  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.19      0.32       361\n",
      "           0       0.10      0.87      0.18        38\n",
      "\n",
      "    accuracy                           0.26       399\n",
      "   macro avg       0.52      0.53      0.25       399\n",
      "weighted avg       0.85      0.26      0.31       399\n",
      "\n",
      "----------logistic\n",
      "accuracy:\n",
      " 90.47619047619048\n",
      "[[  0  38]\n",
      " [  0 361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      1.00      0.95       361\n",
      "           0       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.45      0.50      0.48       399\n",
      "weighted avg       0.82      0.90      0.86       399\n",
      "\n",
      "----------tanh\n",
      "accuracy:\n",
      " 58.89724310776943\n",
      "[[ 13  25]\n",
      " [139 222]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.61      0.73       361\n",
      "           0       0.09      0.34      0.14        38\n",
      "\n",
      "    accuracy                           0.59       399\n",
      "   macro avg       0.49      0.48      0.43       399\n",
      "weighted avg       0.82      0.59      0.67       399\n",
      "\n",
      "----------relu\n",
      "accuracy:\n",
      " 90.47619047619048\n",
      "[[  0  38]\n",
      " [  0 361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      1.00      0.95       361\n",
      "           0       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.45      0.50      0.48       399\n",
      "weighted avg       0.82      0.90      0.86       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scale minmax\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print('----------identity')\n",
    "print('accuracy:\\n', accuracy_score(y_test, y_pred_iden_m)*100)\n",
    "print(confusion_matrix(y_test, y_pred_iden_m, labels=[1, 0]).T)\n",
    "print(confusion_matrix(y_test, y_pred_iden_m, labels=[0, 1]).T)\n",
    "# print(pd.crosstab(y_test, y_pred_iden_m))\n",
    "print(classification_report(y_test, y_pred_iden_m, target_names=['1','0']))\n",
    "\n",
    "print('----------logistic')\n",
    "print('accuracy:\\n', accuracy_score(y_test, y_pred_logi_m)*100)\n",
    "print(confusion_matrix(y_test, y_pred_logi_m, labels=[1, 0]))\n",
    "print(classification_report(y_test, y_pred_logi_m, target_names=['1','0']))\n",
    "\n",
    "print('----------tanh')\n",
    "print('accuracy:\\n', accuracy_score(y_test, y_pred_tanh_m)*100)\n",
    "print(confusion_matrix(y_test, y_pred_tanh_m, labels=[1, 0]))\n",
    "print(classification_report(y_test, y_pred_tanh_m, target_names=['1','0']))\n",
    "\n",
    "print('----------relu')\n",
    "print('accuracy:\\n', accuracy_score(y_test, y_pred_relu_m)*100)\n",
    "print(confusion_matrix(y_test, y_pred_relu_m, labels=[1, 0]))\n",
    "print(classification_report(y_test, y_pred_relu_m, target_names=['1','0']))\n",
    "\n",
    "#          0   1\n",
    "# True    33   291\n",
    "# False   5    70\n",
    "\n",
    "# 0.1 = 33/(33+291)\n",
    "# 0.87 = 33/(33+5)\n",
    "\n",
    "#          1   0\n",
    "# True    70   5\n",
    "# False   291  33\n",
    "\n",
    "# 0.93 = 70/(70+5) \n",
    "# 0.19 = 70/(70+291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
